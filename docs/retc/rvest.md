# rvestでスクレイピング {#rvest}

## スクレイピング

スクレイピングとは，ウェブスクレイピングの省略のことで，ウェブサイトにある情報を収集することである．
ウェブサイトから植生調査データを収集することは少ないものの，関連データの収集は可能である．
例えば，気象庁のページから気象データが収集可能である．
手動でも可能だが多大な手間と時間が必要で，研究に必要なデータを自動で取得できれば，手間と時間の節約が可能である．

ここではウェブでの情報収集の方法を紹介する．
具体的には気象庁のページから世界各地の気象データを入手する．
情報収集にはRのパッケージであるrvestを用いる．
<!--
rvestを用いて気象庁のページから世界の気象データを入手して，気候ダイアグラムを描画する．
-->

## rvest と RSelenium

スクレイピングをするために使われる主なRのパッケージとしては，rvestとRSeleniumがある．
rvestは，静的なサイトを対象とするときに役立つ．
つまり，URLを指定すれば対象のサイトのページが決まるときである．
気象庁での気象データを提供しているページがこれに当たる．
一方，RSeleniumは動的なサイトを対象とするときに役立つ．
例えば，テキストボックスへのデータ入力やプルダウンメニューの選択あるいはその後のマウス操作でページが遷移する場合だ．
このような動的なサイトでは，Seleniumだけでなく，Javascriptを部分的に用いるのも効果的である．
なお，rvestでもユーザ名とパスワードを用いた一般的なログインは可能で，politeパッケージと組み合わせることである程度の動的なサイトのスクレイピングは可能である．

<!--
CRANには，植生データの取得のためのパッケージがある．
  # 本当?
  # 海外のもの
しかし，自分のもとめるデータがあるとは限らない．
特に日本のデータの取得は少ない．

--> 

## rvestのできること

- HTMLの取得    
- DOMの取得: id, class, tagNameなどを用いる   
- tableの取得    
  - HTML内の取得したいデータはtableにあることが多いため，非常に便利   
  そもそも，tableでないデータを取得するのは非常に不便   
- リンクの取得  
  ページ遷移に使用する   
  - stringrと組み合わせて使うと良い   
  - 文字コードの変換にはstringiを用いる   
  - tidyverseやmagrittrとの合せ技が便利    
- Formの入力・選択
  - radioボタンはちょっと工夫が必要   
  -` moranajp::html_radio_set()`   
    無理やりな感じではあるが，同一名称のradioボタンを全て同じ値に変更する   
    本来なら，不要なradioボタンのフォームを削除   
      可能だが，インデクスがずれるので結構厄介  
- politeパッケージとの連携   
  使えば便利だが，ここでは説明せず

## 準備

例によってrvestをインストールする．
curlとpoliteパッケージは少しだけ使うので予めインストールしておく．
tidyverseは既にインストールしているはずだが，まだの場合はインストールする．


```r
install.packages("rvest")
install.packages("curl")
install.packages("polite")
  # install.packages("moranajp")
  # install.packages("tidyverse") # 未インストールの場合
```


```r
library(rvest)
library(tidyverse)
```

## HTMLの取得

スクレイピングによってデータを取得するには，取得したいページのURLを特定しなければならない．
静的なページのURLであれば，ブラウザのアドレスバーにあるURLをそのまま使えば良い．
動的あるいは特定の規則に従ったURLであれば，取得したいページのURLの規則性を知らなければならない．

ここでは，「日本のレッドデータ検索システム」から都道府県のRDB指定状況とその地図情報の画像を入手することを考える．

http://jpnrdb.com/search.php?mode=spec   

まずはブラウザでページにアクセス，手作業で検索，指定状況とその地図情報の画像を入手してみる．
上記URLで例として示されているニッコウキスゲをキーワード(種名)として入力すると，ページが遷移する．
アドレスバーにはカタカナがそのまま表示されている．
しかし，アドレスをコピーしてテキストエディタに貼り付けると文字化けしたようになる．
<!--
http://jpnrdb.com/search.php?mode=key&q=ニッコウキスゲ   
http://jpnrdb.com/search.php?mode=key&q=%E3%83%8B%E3%83%83%E3%82%B3%E3%82%A6%E3%82%AD%E3%82%B9%E3%82%B2
-->
これはURLエンコードによってコード変換された結果であるが安心して欲しい．
rvestを使ってHTMLを取得するときには，日本語をそのまま使用することができる．
上記のURLのうち「 http://jpnrdb.com/search.php?mode= 」まではここで使用するページに共通する部分であるため，mainとしておく．
検索したい種名は変更する部分で，とりあえずspに入れておく．
キーワード検索の命令(phpによるクエリ)と種名の文字列を結合し，さらにmainと結合する．
これで得たURLを`read_html()`に与えると，ページのHTMLを得ることができる．


```r
main <- "http://jpnrdb.com/search.php?mode="
sp <- "ニッコウキスゲ"
find_sp <- paste0("key&q=", sp)
html <- 
  paste0(main, find_sp) %>%
  rvest::read_html()
html
```

```
## {html_document}
## <html>
## [1] <body>\n<em></em>\n\n<title>日本のレッドデータ検索システム</title>\n<meta http-equiv="co ...
```

ところで，手作業にはなってしまうが，ニッコウキスゲを検索した結果のページをブラウザで表示させ，その後のページの内容を確認する．
ウエブページには次の規則性があることに気づく．
検索結果のページには表(table)としてデータが含まれており，その表の中の目録Noである「5259」が指定状況や地図のページのURLに含まれている．
つまり，目録Noを入手すれば指定状況や地図ページのURLを生成できそうだ．


## 必要な情報の取得

`read_html()`で取得したHTMLには必要な情報が含まれているが，そのままの状態では使い物にならない．
また，文字列に変換してstringrを駆使すれば，情報を得ることはできるだろうが，多大な苦労が待っている．

HTMLの全体を表示させたい場合は，以下のコードを実行する．
とてもではないが，これを自分で解析したいとは思わないだろう．


```r
 # 全体を表示させたい場合
as.character(html) %>%
  cat()
```

最初と最後の部分だけを示すと次のようになる．


```r
as.character(html) %>%
  stringr::str_split("\\n") %>%
  `[[`(1) %>%
  head()
```

```
## [1] "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">"
## [2] "<html><body>"                                                                                                  
## [3] "<em></em>"                                                                                                     
## [4] ""                                                                                                              
## [5] "<title>日本のレッドデータ検索システム</title>"                                                                 
## [6] "<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">"
```

```r
as.character(html) %>%
  stringr::str_split("\\n") %>%
  `[[`(1) %>%
  tail()
```

```
## [1] "}"              ""               "//-->"          "</script>"     
## [5] "</body></html>" ""
```

幸いにしてrvestにはhtmlから要素を取得するための便利な関数が用意されている．
ウエブページを見ると，tableにデータが入っていそうなので，`html_table()`でデータを抽出する．
ここではスペースの節約のため`purrr::map(colnames)`として列名だけ表示するとともに不要なスペースを除去している．
なお，`rvest::html_table(html)`とするとtable全体を表示できる．


```r
rvest::html_table(html) %>%
  purrr::map(colnames) %>%
  purrr::map(stringr::str_remove, " ")
```

```
## [[1]]
## [1] "X1"
## 
## [[2]]
## [1] "X1"
## 
## [[3]]
## [1] "X1" "X2" "X3" "X4" "X5" "X6"
## 
## [[4]]
##  [1] "X1"  "X2"  "X3"  "X4"  "X5"  "X6"  "X7"  "X8"  "X9"  "X10" "X11" "X12"
## 
## [[5]]
## [1] "X1" "X2"
## 
## [[6]]
## [1] "目録No▲▼"                                            
## [2] "上位分類群▲▼"                                        
## [3] "科名▲▼"                                              
## [4] "和名▲▼／学名\r\n\t\t                            ▲▼"
## [5] "指定都道府県数▲▼"                                    
## [6] "環境省▲▼"                                            
## 
## [[7]]
## [1] "X1" "X2"
## 
## [[8]]
## [1] "X1" "X2"
```


列名から6個目のtableが目録Noを含んでいそうだと考えられるので，6つ目の表を表示させる．


```r
rvest::html_table(html) %>%
  `[[`(6)
```

```
## # A tibble: 2 × 6
##   `目録No▲▼` `上位分類群▲▼` `科名▲▼` 和名▲▼／学名\r\n\t\t…¹   `指定都道府県数▲▼`
##        <int> <chr>          <chr>    <chr>                                 <int>
## 1       5266 単子葉類       ユリ     ゼンテイカ  Hemerocalli…                  6
## 2       5259 単子葉類       ユリ     ニッコウキスゲ  Hemeroc…                  3
## # ℹ abbreviated name: ¹​`和名▲▼／学名\r\n\t\t                             ▲▼`
## # ℹ 1 more variable: `環境省▲▼` <chr>
```

和名としてゼンテイカとニッコウキスゲの2つが示されている．
ゼンテイカはニッコウキスゲの別名である．
ほぼ同じものなので，本来は両方の情報を合わせる必要がある．
別名かどうか判定するには生物の種について考える必要があり，この問題はかなり根深くてややこしいため，ここではあえて立ち入らない．
単純に検索したものと同じ文字列の和名の目録Noを得ることを考える．

tableの列名とその内容をもとにして目録Noを取得するには，dplyrのselectとfilterが便利だ．
selectは列名を指定する以外に列番号を指定できるので，それを使う．
さらに，filter，stringr，stringiの関数の合せ技でspと同じ文字列のnoを取り出す．

途中にちょっと面倒な点があるので，その点だけやや詳しく補足する．
`separate()`の`sep`(区切り文字)として，`stringi::stri_unescape_unicode("\\u00a0")`を指定している．
これは，普通の半角スペースに見えるが，No-Break Spaceと言われる改行を防ぐ特殊なスペースである．
これをそのままコードに入力しても良いが，どう見ても普通のスペースと見分けがつかない．
後からコードを書く時に普通のスペースを使ってしまうと，区分しようとしてもうまくいかない．
そこで，これは普通のスペースではないことを明示的に示した．
また，`str_detect()`の引数で，`paste0("^", sp, "$")`としたのは，「ニッコウキスゲ」以外にマッチさせないためである．
例えば，「ギンラン」を検索すると，「ギンラン」以外にも「エゾギンラン」と「ササバギンラン」も出てくる．
この場合に正規表現の`^`(行頭の意味)`$`(行末の意味)を使うことで，「ギンラン」にしかマッチさせない．


```r
no <- 
  rvest::html_table(html) %>%
  `[[`(6) %>%
  dplyr::select(no = 1, wamei = 4) %>%
  tidyr::separate(wamei, into = "wamei", sep = stringi::stri_unescape_unicode("\\u00a0"), extra = "drop") %>%
  dplyr::filter(stringr::str_detect(wamei, paste0("^", sp, "$"))) %>%
  `[[`("no")
```

## URLの生成・データの取得

目録Noが取得できれば，完成したようなものである．
ブラウザで表示した地図や指定状況のURLを生成する．
"map&q=0605009"の詳細な意味はよくわからないが，"0605009"あたりは分類群を指定しているのだと考えられる．
これに維管束植物(コケなどを除くシダ植物と花の咲く植物)の中での目録Noを結合して，さらにmainを結合するとURLの出来上がりだ．


```r
show_sp <- paste0("map&q=0605009", no)
paste0(main, show_sp)
```

```
## [1] "http://jpnrdb.com/search.php?mode=map&q=06050095259"
```

```r
html <- 
  paste0(main, show_sp) %>%
  rvest::read_html()
```


生成したURLをブラウザで表示させると地図ページが表示される．
一覧表の表示にしてもURLは変更されないため，内部的に表示を変更させている可能性が高い．
そこで，とりあえずHTMLからtableデータの列名を確認する．
なお，`str_remove_all()`で不要な文字の除去をしている．


```r
html %>%
  rvest::html_table() %>%
  purrr::map(colnames) %>%
  purrr::map(stringr::str_remove_all, "\\n|\\t|\\r")
```

```
## [[1]]
## [1] "X1"
## 
## [[2]]
## [1] "X1"
## 
## [[3]]
## [1] "X1" "X2" "X3" "X4" "X5" "X6"
## 
## [[4]]
## [1] "都道府県名▲▼" "和名"           "学名"           "RDBカテゴリ名" 
## [5] "統一カテゴリ"  
## 
## [[5]]
##  [1] "ニッコウキスゲ学名：Hemerocallis dumortieri var. esculenta分類： 単子葉類    ユリ科 登録別名：環境省カテゴリ：なし都道府県のRDB指定状況："
##  [2] "ニッコウキスゲ"                                                                                                                           
##  [3] "ニッコウキスゲ"                                                                                                                           
##  [4] "学名："                                                                                                                                   
##  [5] "Hemerocallis dumortieri var. esculenta"                                                                                                   
##  [6] "分類："                                                                                                                                   
##  [7] "単子葉類    ユリ科"                                                                                                                       
##  [8] "登録別名："                                                                                                                               
##  [9] ""                                                                                                                                         
## [10] "環境省カテゴリ："                                                                                                                         
## [11] "なし"                                                                                                                                     
## [12] "都道府県のRDB指定状況："                                                                                                                  
## [13] ""                                                                                                                                         
## 
## [[6]]
## [1] "X1" "X2"
```

果たして，tableの4番目に欲しいデータがあった．
あとは，filterで不要な都道府県データを除去する．
さらに，データを分析するならば整形・変換・保存などをするが，スクレイピングとしての作業はここまでとする．


```r
html %>%
  rvest::html_table() %>%
  `[[`(4) %>%
  dplyr::filter(和名 != "-")
```

```
## # A tibble: 4 × 5
##   `都道府県名▲▼`                        和名   学名   RDBカテゴリ名 統一カテゴリ
##   <chr>                                 <chr>  <chr>  <chr>         <chr>       
## 1 "埼玉県\r\n        ※"                 "ニッ… "Heme… "絶滅危惧Ⅱ類… ""          
## 2 "滋賀県"                              "ニッ… "Heme… "分布上重要…  ""          
## 3 "島根県"                              "ニッ… "Heme… "絶滅危惧Ⅰ類… ""          
## 4 "※埼玉県・東京都・神奈川県では、季節… "※埼…  "※埼…  "※埼玉県・…   "※埼玉県・…
```

## 地図画像の取得

指定状況の地図画像を取得するには，まずブラウザで画像のURLを得る必要がある．
GoogleChromeで画像を右クリックして，「画像アドレスをコピー」を選択する．
ニッコウキスゲの場合は，以下のURLを得ることができる．

http://jpnrdb.com/png/06/06050095259.png

指定状況の一覧表データのHTMLにも(ほぼ)同じものが含まれているはずである．
rvestで目的とするファイルのURLを得るコードは以下のとおりである．


```r
html %>%
  rvest::html_elements("img") %>%
  rvest::html_attr("src") %>%
  `[`(., stringr::str_detect(. , as.character(no)))
```

```
## [1] "./png/06/06050095259.png"
```

上のコード使用したように，rvestで便利な関数としてhtml_elements()とhtml_attr()がある．
それぞれ次のようにid，class，tag，属性によってHTMLからDOMを取得可能である．

- html_elements()   
  - html_elements("#id")   
  - html_elements(".class")   
  - html_elements("tag")    
- html_attr("attribute")   

DOMとはドキュメントオブジェクトモデルのことで，HTMLの各要素をオブジェクトとするモデルのことである．
id，class，tag，属性を指定することで，効率的にオブジェクトを取り出せて便利である．

id，class，tag，属性についての詳細は，HTMLの解説などを別途参照していただきたい．
簡単に説明をすると，idはHTML内で一意に決定できるもので，日本のレッドデータ検索システムでは<id = "header">などが使われている．
classは，HTML内で複数出てくることがあり，`<class = "kind_list">`のように指定される．
tagは，上記の`<id>`や`<class>`を含めたすべてのタグのことで，他にも`<p>`，`<div>`，`<table>`など多くの物がある．
`html_table()`は`html_elements("table")`と同等であるが，tableタグは入手したいデータを含むことが多いため個別の関数が作成されたのだろう．
属性はtagの，「href = "index.html"」の部分で，`html_attr("href")`とすると，"index.html"を取り出すことができる．
hrefが複数ある場合は，すべてを含むベクトルが返り値になる．


ただし実際には，上のようにブラウザでの右クリックか，以下の手順で実行するのが手っ取り早い．
- ブラウザで地図ページを表示させる   
- F12を押して開発者ツールを開く   
- 左上の□と矢印の結合したアイコンをクリック後に画像をクリック   
- Elementsのところに出てきたURLが求めるURL   
- タグを右クリックして[Copy] - [Copy element] や [Copy outerHTML] で内容をコピーできる  

画像のURLがわかれば，ファイルをダウンロードして保存する．
これは，curlパッケージの`curl_download()`で簡単にできる．
引数urlにはURLを，destfileにはダウンロード後のファイル名を指定する．
パスを指定しないと作業ディレクトリ(`getwd()`で取得可能)に保存されるが，作業ディレクトリ以外に保存したい場合は，相対パスや絶対パスを指定する．


```r
  # wd <- "set_your_directory"
  # setwd(wd)
url_img <- "http://jpnrdb.com/png/06/06050095259.png"
curl::curl_download(url = url_img, destfile = paste0("img/", sp, ".png"))
```

このようにしてスクレイピングが可能ではあるが，URLの生成規則は，変更されることがある．
`read_html()`でHTMLが取得できない場合は，URLを確認する．
また，動的なサイトでは，idが固定ではない可能性がある．
サイトの仕様変更によって，tag，class，その他の構造が変更されることがある点も注意しなければならない．

気象庁ではなく他のサイトの事例ではあるが，綺麗な構造のサイトであっても，手作業が混入していることはある．
例えば，括弧が正しく対応しているはずだと思っていても，開く側が"『"で閉じる側が"」"になっていることがあった．
その場合に正規表現"『.+』"ではうまく鉤括弧内の文字列を取得できないことになる．

## 複数種への対応

前節のようにすれば，レッドデータへの指定状況とその地図データを得ることができる．
1種だけのデータ・画像の入手方法を紹介したが，複数種に対しても応用可能である．
その際には，forループか，purrr::mapを使うと良いだろう．

複数ページのデータを取得する場合は一般的には5秒程度の間隔を置く必要がある．
ただし，サイトによってはそれ以上の間隔を求めているときがある．
その内容はドメインのトップに置かれた「robots.txt」で確認できる．
"http://jpnrdb.com/"には"robots.txt"が置かれていないが，politeパッケージの関数`bow()`でスクレイピングについて調べてみる．


```r
polite::bow("http://jpnrdb.com/")
```

```
## <polite session> http://jpnrdb.com/
##     User-agent: polite R package
##     robots.txt: 1 rules are defined for 1 bots
##    Crawl delay: 5 sec
##   The path is scrapable for this user-agent
```

「Crawl delay: 5 sec」とあるため，5秒間隔でスクレイピング可能であると思われる．
これ未満の間隔でデータを頻繁に求めると，「攻撃」と見なされて接続できな状態になる可能性がある．
さらに悪質なときには法的手段を取られることもありえるので，注意が必要である．
れらはpoliteパッケージが一般的な注意として示しているに過ぎない．
大量にデータを入手する必要がある場合は，あらかじめ管理者に連絡する方が無難である．


## おまけ：webshotでウエブページを画像に変換 {#webshot}

スクレイピングしたウエブページを画像として残しておきたいことがある．
つまり，ウエブのスクリーンショットを保存したい場合だ．
まさにこの文章を書いているときがそうだが，データを入手してそのサイトについて他人に説明したいときがあるだろう．
そのようなときはパッケージwebshotが便利だ．

例によってまずはパッケージをインストールする．


```r
install.packages("webshot")
```



webshotは内部でPhantomjsというブラウザを使っているので，
webshotからPhantomjsをインストールするための関数を実行する．
なお，Phantomjsはヘッドレス・ブラウザの1つである．
ヘッドレス・ブラウザは，画面を描画しないブラウザのことである．
つまり，画面上でHTMLを表示せずにデータのやり取りだけをするもので，プログラムやスクレイピングでは重宝する．
<!--
画面表示がないため動作が早い．
-->


```r
webshot::install_phantomjs()
```

イントールに若干時間がかかるが，終わればあとは簡単だ．
関数webshotにURLと保存するファイル名を指定すれば，画像を作業フォルダに保存してくれる．


```r
webshot::webshot("http://jpnrdb.com/search.php?mode=spec", "img/rvest_1.png")
webshot::webshot("http://jpnrdb.com/search.php?mode=key&q=ニッコウキスゲ", "img/rvest_2.png")
webshot::webshot("http://jpnrdb.com/search.php?mode=map&q=06050095259", "img/rvest_3.png")
```

なお，パッケージmagickを使うと保存した画像に対してトリミングなどの加工ができる．

参考：[magickで画像編集](#magick)
